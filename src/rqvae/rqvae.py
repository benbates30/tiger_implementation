import torch
from torch import nn
from torch.nn import functional as F
import numpy as np
from typing import List, Dict


"""
TIGER RQ-VAE info:

- The encoder has three
intermediate layers of size 512, 256 and 128 with ReLU activation,
with a final latent representation dimension of 32.

-  3 levels of residual quantization, seperate codebooks with cardinality of 256,
each vector in codebook has dimensionality of 32

- Beta of 0.25 used for loss

- Trained for 20k epochs, achieves >= 80% codebook usage. Adagrad Optimizer with lr=0.4.
Batch size = 1024

- To prevent codebook collapse, k-means clustering based initialization is used for the codebook.
k-means is applied on first training batch, with centroids used for initalization.
"""

class Encoder(nn.Module):
    """
    DNN Encoder to encode the input semantic embedding into a latent representation.
    Following the use of this encoder, the output will be fed through a residual quantizer to
    get a quantize representation (Sematic ID), which can then be fed through the Decoder with 
    the aim of reconstructing the original semantic embedding that was passed as input to this
    encoder.
    """

    def __init__(self, 
                in_channels: int, 
                hidden_channels: List = None,
                latent_dim: int = 32,
                ):
        super().__init__()


        self.stages = nn.ModuleList()

        if hidden_channels is None:
            hidden_channels = [512, 256, 128]

        last_dim = in_channels
        for dim in hidden_channels:
            stage = nn.Sequential(
                nn.Linear(last_dim, dim),
                nn.ReLU(),
            )
            self.stages.append(stage)
            last_dim = dim
        
        self.stages.append(nn.Sequential(
            nn.Linear(last_dim, latent_dim),
            nn.ReLU(),
        ))

    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for stage in self.stages:
            x = stage(x)
        return x
        





class Decoder(nn.Module):
    """
    DNN Decoder that decodes quantized represenations (Semantic IDs) that are generated by
    passing the semantic embedding through the encoder then the residual quantizer back to the
    original semantic embedding that represents the item using contextual information.
    """

    def __init__(self,
                out_channels: int,
                hidden_channels: List[int] = None,
                latent_dim: int = 32,
                ) -> None:
        super().__init__()


        if hidden_channels is None:
            hidden_channels = [128, 256, 512]
        
        self.stages = nn.ModuleList()
        last_dim = latent_dim
        for dim in hidden_channels:
            stage = nn.Sequential(
                nn.Linear(last_dim, dim),
                nn.ReLU(),
            )
            self.stages.append(stage)
            last_dim = dim
        
        
        self.stages.append(nn.Sequential(
            nn.Linear(last_dim, out_channels),
            nn.ReLU(),
        ))
        
        


    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for stage in self.stages:
            x = stage(x)
        
        return x



class VQEmbedding(nn.Embedding):
    """
    Vector Embeddings to be used for residual quantization
    
    Currently tested everything except the distances method (assumed to be correct)
    """

    def __init__(self,
                n_embed: int,
                embed_dim: int,
                ) -> None:
        
        super().__init__(n_embed + 1, embed_dim, padding_idx=n_embed)

        self.n_embed = n_embed
        self.embed_dim = embed_dim

    @torch.no_grad()
    def compute_distances(self, inputs):
        # Inputs should be of shape [B, embed_dim]
        codebook_t = self.weight[:-1, :].t()
        (embed_dim, _) = codebook_t.shape
        assert inputs.shape[-1] == embed_dim # embeddings being compared but be same size
        
        inputs_norm_sq = inputs.pow(2.).sum(dim=1, keepdim=True)
        codebook_t_norm_sq = codebook_t.pow(2.).sum(dim=0, keepdim=True)
        distances = torch.addmm(
            inputs_norm_sq + codebook_t_norm_sq,
            inputs, 
            codebook_t,
            alpha=-2.0
        )

        return distances

    
    @torch.no_grad()
    def find_nearest_embedding(self, inputs):
        distances = self.compute_distances(inputs) # [B, n_embed or n_embed+1]
        embed_idxs = distances.argmin(dim=-1)
        return embed_idxs

    def forward(self, inputs):
        embed_idxs = self.find_nearest_embedding(inputs)

        embeds = self.embed(embed_idxs)
        
        return embeds, embed_idxs

    def embed(self, idxs):
        embeds = super().forward(idxs)
        return embeds
        


class RQBottleneck(nn.Module):
    """
    Residual quantizer to serve as bottleneck of RQVAE. This quantizes the latent space
    generated by passing the contextual embedding through the encoder into a Semantic ID.
    """
    
    def __init__(self,
                num_codebooks: int,
                embed_dim: int,
                codebook_size: int,
                shared_codebook: bool = False
                ) -> None:
        
        super().__init__()

        # TODO: check embed dim done properly

        self.num_codebooks = num_codebooks
        self.embed_dim = embed_dim
        self.shared_codebook = shared_codebook

        self.codebook_size = [codebook_size for _ in range(self.num_codebooks)] # all codebooks are same size

        if self.shared_codebook: # not what's used by TIGER
            self.codebooks = nn.ModuleList([VQEmbedding(
                self.codebook_size[0], self.embed_dim
                ) for _ in range(self.num_codebooks)])

        else:
            self.codebooks = nn.ModuleList([VQEmbedding(
                self.codebook_size[idx], self.embed_dim
                ) for idx in range(self.num_codebooks)])
    

    def quantize(self, x):
        """
        Returns indices of quantized emebeddings along with the actual embedding that they map to.

        
        Args:
            x (Tensor): Feature embeddings to quantize
        Returns:
            quant_embeds (list): A list of the embedding results from each codebook (size of dim=0
            is number of codebooks)
            codes (list): A list of the codes produced from each code book (size of dim=0 is 
            number of codebooks)
        Shape:
        x: [B, embed_dim]
        quant_embeds[i]: [B, embed_dim]
        codes: [B, embed_dim]
        """

        B, embed_dim = x.shape

        residual_feature = x.detach().clone()

        quant_embeds = []
        codes = []
        agg_quants = torch.zeros_like(x)

        for i in range(self.num_codebooks):
            quant_emb, code = self.codebooks[0](residual_feature)

            residual_feature.sub_(quant_emb) # r[d] = r[d-1] - e(k[d]) 
            agg_quants.add_(quant_emb) # z^d = sum(i=1->d) e(k[i]) // we're adding this embed to the sum

            codes.append(code.unsqueeze(dim=-1))
            quant_embeds.append(agg_quants.clone()) # check this


        codes = torch.cat(codes, dim=-1)
        return quant_embeds, codes
    

    def forward(self, x):
        quant_embeds, codes = self.quantize(x)
        # truncating for embeds here?
        return quant_embeds, codes



class RQVAE(nn.Module):
    """
    Full fledged RQ-VAE model, consisting of a DNN encoder, a residual quantizer bottleneck, and
    a DNN decoder. 
    """

    def __init__(self,
                num_codebooks: int,
                codebook_size: int,
                in_channels: int,
                latent_dim: int,
                hidden_channels: List[int] = None,
                enc_dec_config=None) -> None:
        super().__init__()

        if hidden_channels is None:
            self.hidden_channels = [512, 256, 128]
        
        self.encoder = Encoder(
            in_channels, hidden_channels, latent_dim,)
        self.rquant = RQBottleneck(
            num_codebooks, latent_dim, codebook_size, shared_codebook=False)
        self.decoder = Decoder(
            in_channels, hidden_channels[::-1], latent_dim, 
        )
        # self.encoder = Encoder(**enc_dec_config)
        # self.decoder = Decoder(**enc_dec_config)

        



    
    def forward(self, xs):
        z_e = self.encode(xs)
        z_q, codes = self.rquant(z_e)
        out = self.decode(z_q)
        return out, codes


    def encode(self, x):
        z_e = self.encoder(x)
        return z_e



    def decode(self, z_q):
        if isinstance(z_q, List):
            z_q = z_q[-1]
        out = self.decoder(z_q)
        return out
        





if __name__ == "__main__":
    # testing the model definition and training loop
    
    """
    Encoder-Decoder tests
    """
    # inputs = torch.tensor([[0.04, 0.96, -0.43, -0.65, 0.46], [0.69, -0.34, -0.12, 0.54, -0.87]])
    # encoder = Encoder(5, [4, 3], 2)
    # decoder = Decoder(5, [3, 4], 2)

    # latent = encoder(inputs)
    # print(latent)
    # out = decoder(latent)
    # print(out)


    """
    VQ Embedding tests
    """
    # inputs = torch.tensor([[0.04, 0.96, -0.43, -0.65, 0.46], [0.69, -0.34, -0.12, 0.54, -0.87]])
    # # Initiaization functions properly, including padding
    # # can also retrieve embeddings correctly
    # embed_table = VQEmbedding(10, 5) # 10 embeedings of dimension 5
    # print(embed_table)
    # all_embedddings = embed_table.embed(torch.tensor([i for i in range(11)])) # viewing all the embeddings
    # print(all_embedddings)

    # # forward pass functions.
    # embeds, embed_idxs = embed_table(inputs)
    # print(embed_idxs)
    # print(embeds)

    """
    RQ bottleneck tests
    """
    # inputs = torch.tensor([[0.04, 0.96, -0.43, -0.65, 0.46], [0.69, -0.34, -0.12, 0.54, -0.87]])
    # # initalizes properly
    # rq = RQBottleneck(3, 5, 12)
    # print(rq)
    # # quantizes properly (at least visually)
    # quant_embeds, codes = rq.quantize(inputs)
    # print(codes)
    # print(quant_embeds)

    """
    RQ-VAE tests
    """
    inputs = torch.tensor([[0.04, 0.96, -0.43, -0.65, 0.46], [0.69, -0.34, -0.12, 0.54, -0.87]])

    vae = RQVAE(3, 13, 5, 2, [4, 3])
    print(vae)
    out = vae(inputs)
    print(out)

   

